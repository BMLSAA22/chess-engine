{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import agent\n",
    "from board import board\n",
    "from model import Net\n",
    "# import torch.nn.functional as F\n",
    "# import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import agent\n",
    "from board import board\n",
    "from moves import *\n",
    "from model import Net\n",
    "import torch\n",
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def generate_trajectory(max_t=100):\n",
    "        Agent=agent(copy.deepcopy(board))\n",
    "\n",
    "\n",
    "        states=[]\n",
    "        greedy_moves = []\n",
    "        # predicted_moves=[]\n",
    "        \n",
    "        \n",
    "\n",
    "        for t in range(max_t):\n",
    "            \n",
    "            state=Agent.board_to_array()\n",
    "            states.append(state)\n",
    "            # tensor = torch.from_numpy( state ).float().unsqueeze(0)\n",
    "            # action=actor(tensor).detach().squeeze()\n",
    "            # predicted_moves.append(action)\n",
    "            tag,to,castles = Agent.sample(\"W\")\n",
    "            greedy_move = np.zeros(675)\n",
    "            greedy_move[Agent.abs_to_rel_moves(tag,to,castles)]=1\n",
    "            greedy_moves.append ( greedy_move )\n",
    "            white_reward=Agent.make_move(tag,to,castles)\n",
    "\n",
    "\n",
    "                \n",
    "            if Agent.checkmate(\"B\"):\n",
    "                print(\"white won\")\n",
    "                break\n",
    "            if Agent.stalemate(\"B\"):\n",
    "                \n",
    "\n",
    "                break\n",
    "\n",
    "            # add te obtained results to their relative lists ==> saved_log_probs, rewards, state_values\n",
    "            tag,to,castles=Agent.sample(\"B\")\n",
    "            \n",
    "            black_reward=Agent.make_move(tag,to,castles)\n",
    "            \n",
    "            if Agent.checkmate(\"W\"):\n",
    "                print(\"white won\")\n",
    "                break\n",
    "            if Agent.stalemate(\"B\"):\n",
    "                print(\"black won\")\n",
    "                \n",
    "                break\n",
    "\n",
    "        return   states , greedy_moves \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('states.pkl', 'rb') as file:\n",
    "    states = pickle.load(file)\n",
    "with open('taraget.pkl', 'rb') as file:\n",
    "    target = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# states=[]\n",
    "# target=[]\n",
    "for i in range (200):\n",
    "    print(i)\n",
    "    a,b=generate_trajectory(100)\n",
    "    states+=a\n",
    "    target+=b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x=np.stack(states)\n",
    "y=np.stack(target)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Save variable to a file\n",
    "# with open('states.pkl', 'wb') as file:\n",
    "#     pickle.dump(states, file)\n",
    "# with open('taraget.pkl', 'wb') as file:\n",
    "#     pickle.dump(target, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "input1 = tf.keras.layers.Input(shape=(6,8,8))\n",
    "# shape1 = tf.keras.layers.Reshape(target_shape=(8, 8, 6))(input1)\n",
    "conv1 = tf.keras.layers.Conv2D(kernel_size=(8,8), padding=\"same\", activation=\"relu\", filters=64, input_shape=(8,8,1))(input1)\n",
    "bn1 = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-05)(conv1)\n",
    "conv2 = tf.keras.layers.Conv2D(kernel_size=(8,8), padding=\"same\", activation=\"relu\", filters=64, input_shape=(8,8,1))(bn1)\n",
    "bn2 = tf.keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-05)(conv2)\n",
    "flatten1 = tf.keras.layers.Flatten()(bn2)\n",
    "# input2 = tf.keras.layers.Input(shape=(5,))\n",
    "\n",
    "# conc = tf.keras.layers.concatenate([flatten1,input2])\n",
    "\n",
    "Denselayer1 = tf.keras.layers.Dense(512, activation='relu')(flatten1)\n",
    "# Denselayer2 = tf.keras.layers.Dense(512, activation='relu')(Denselayer1)\n",
    "# Denselayer3 = tf.keras.layers.Dense(256, activation='relu')(Denselayer2)\n",
    "# Denselayer4 = tf.keras.layers.Dense(256, activation='relu')(Denselayer3)\n",
    "Output = tf.keras.layers.Dense(675, activation='softmax')(Denselayer1)\n",
    "\n",
    "\n",
    "\n",
    "data_model = tf.keras.models.Model(inputs=input1, outputs=Output)\n",
    "\n",
    "# predictions = data_model([(inputboard[:1]), (inputmeta[:1])]).numpy\n",
    "\n",
    "metric =[tf.keras.metrics.CategoricalCrossentropy()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Removing the Clipnorm of clipnorm=1 may make training faster\n",
    "#opt = tf.keras.optimizers.Adam(clipnorm=1)\n",
    "\n",
    "opt = tf.keras.optimizers.Adam()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "los = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "data_model.compile(optimizer=opt,\n",
    "                   loss=los,\n",
    "                   metrics=metric)\n",
    "data_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "los = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "data_model.compile(optimizer=opt,\n",
    "                   loss=los,\n",
    "                   metrics=metric)\n",
    "data_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_model.fit(x, y, epochs=100, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import random\n",
    "# loss_hist=[]\n",
    "# n=len(states)\n",
    "# for i in range(10000):\n",
    "# # Sample corresponding values from both lists using zip\n",
    "#     sampled_indices = np.random.choice(n, 1, replace=False)\n",
    "#     # print(sampled_indices)\n",
    "# # Unzip the sampled pairs to get separate lists\n",
    "#     sampled_state =np.stack(states)[sampled_indices]\n",
    "#     sampled_target = np.stack(target)[sampled_indices]\n",
    "#     sampled_state=torch.Tensor(sampled_state)\n",
    "#     sampled_target=torch.Tensor(sampled_target)\n",
    "#     pred=model(sampled_state.float())\n",
    "\n",
    "#     loss=train(optimizer , pred , sampled_target)\n",
    "#     if (i % 100)==0:print(f\"loss in {i}th iteration is {loss}\")\n",
    "#     loss_hist.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "loss_hist=[i.item() for i in loss_hist]\n",
    "scores= pd.Series(loss_hist, name=\"scores_Actor\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "_ = scores.plot(ax=ax, label=\"scores_Actor\")\n",
    "_ = (scores.rolling(window=100)\n",
    "           .mean()\n",
    "           .rename(\"Rolling Average\")\n",
    "           .plot(ax=ax))\n",
    "ax.legend()\n",
    "_ = ax.set_xlabel(\"Episode Number\")\n",
    "_ = ax.set_ylabel(\"scores_Actor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import agent\n",
    "from board import board\n",
    "from moves import *\n",
    "from model import Net\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def generate_trajectory(actor, max_t=100):\n",
    "        Agent=agent(copy.deepcopy(board))\n",
    "\n",
    "        # critic=Critic()\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state_values=[]\n",
    "        # print(\"generate trajectory\")\n",
    "        \n",
    "        \n",
    "\n",
    "        for t in range(max_t):\n",
    "            Agent.visualize()\n",
    "            \n",
    "            state=torch.tensor(Agent.board_to_array()).float().unsqueeze(0)\n",
    "            mask,mp=Agent.rel_to_abs_moves()\n",
    "            #state_val= critic(state)\n",
    "            # try:\n",
    "            action_id =  actor.forward(state)\n",
    "            print(action_id)\n",
    "\n",
    "            #      print('no')\n",
    "            #      break\n",
    "                \n",
    "            # action=mp[action_id]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # white_reward= Agent.make_move(action['tag'],action['to'],action['castles'])\n",
    "\n",
    "            # saved_log_probs.append(log_prob)\n",
    "            if Agent.checkmate(\"B\"):\n",
    "                rewards.append(30)\n",
    "                print(\"white won\")\n",
    "                break\n",
    "            if Agent.stalemate(\"B\"):\n",
    "                \n",
    "                rewards.append(-2)\n",
    "                break\n",
    "\n",
    "            # add te obtained results to their relative lists ==> saved_log_probs, rewards, state_values\n",
    "            tag,to,castles=Agent.sample(\"B\")\n",
    "            \n",
    "            black_reward=Agent.make_move(tag,to,castles)\n",
    "            \n",
    "            if Agent.checkmate(\"W\"):\n",
    "                rewards.append(-30)\n",
    "                print(\"black won\")\n",
    "                break\n",
    "            if Agent.stalemate(\"W\"):\n",
    "                \n",
    "                rewards.append(2)\n",
    "                break\n",
    "\n",
    "            # add code here\n",
    "            \n",
    "            # # add code here\n",
    "            # rewards.append(white_reward - black_reward)\n",
    "           \n",
    "            # add code here\n",
    "\n",
    "        # Agent.visualize()\n",
    "        return  saved_log_probs , rewards , state_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agent=agent(board)\n",
    "Agent.visualize()\n",
    "pred=model(torch.tensor(Agent.board_to_array()).float().unsqueeze(0)).squeeze().detach().numpy()\n",
    "print(pred)\n",
    "id=np.argmax(pred)\n",
    "_,a=Agent.rel_to_abs_moves(color=\"W\")\n",
    "a[id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = np.random.choice(5, size=2, replace=False)\n",
    "np.stack([5,2,1,2,3,5,4,5])[sampled_indices ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ord('9')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
