{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent import agent\n",
    "from board import board\n",
    "from moves import *\n",
    "from model import Net\n",
    "import torch\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def generate_trajectory(actor,critic, max_t=100):\n",
    "        Agent=agent(copy.deepcopy(board))\n",
    "\n",
    "        # critic=Critic()\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state_values=[]\n",
    "        # print(\"generate trajectory\")\n",
    "        \n",
    "        \n",
    "\n",
    "        for t in range(max_t):\n",
    "            Agent.visualize()\n",
    "            \n",
    "            state=Agent.board_to_array()\n",
    "            mask,mp=Agent.rel_to_abs_moves()\n",
    "            #state_val= critic(state)\n",
    "            # try:\n",
    "            action_id, log_prob =  actor.sample(state,mask)\n",
    "            state_value=critic(state)\n",
    "            state_values.append(state_value)\n",
    "            # except:\n",
    "            #      print('no')\n",
    "            #      break\n",
    "                \n",
    "            action=mp[action_id]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            white_reward= Agent.make_move(action['tag'],action['to'],action['castles'])\n",
    "\n",
    "            saved_log_probs.append(log_prob)\n",
    "            if Agent.checkmate(\"B\"):\n",
    "                rewards.append(30)\n",
    "                print(\"white won\")\n",
    "                break\n",
    "            if Agent.stalemate(\"B\"):\n",
    "                \n",
    "                rewards.append(-2)\n",
    "                break\n",
    "\n",
    "            # add te obtained results to their relative lists ==> saved_log_probs, rewards, state_values\n",
    "            tag,to,castles=Agent.sample(\"B\")\n",
    "            \n",
    "            black_reward=Agent.make_move(tag,to,castles)\n",
    "            \n",
    "            if Agent.checkmate(\"W\"):\n",
    "                rewards.append(-30)\n",
    "                print(\"black won\")\n",
    "                break\n",
    "            if Agent.stalemate(\"W\"):\n",
    "                \n",
    "                rewards.append(2)\n",
    "                break\n",
    "\n",
    "            # add code here\n",
    "            \n",
    "            # add code here\n",
    "            rewards.append(white_reward - black_reward)\n",
    "           \n",
    "            # add code here\n",
    "\n",
    "        # Agent.visualize()\n",
    "        return  saved_log_probs , rewards , state_values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "#Using a neural network to learn state value\n",
    "class Critic(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(Critic, self).__init__()\n",
    "            self.conv1 = nn.Conv2d(6, 32, kernel_size=3, stride=1, padding=1)\n",
    "            self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "            self.fc1 = nn.Linear(256, 128)\n",
    "            self.fc2 = nn.Linear(128, 64)\n",
    "            self.fc3 = nn.Linear( 64, 1)\n",
    "            self.flatten=nn.Flatten()\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = torch.from_numpy(x).float().unsqueeze(0)\n",
    "            x = self.pool(self.conv1(x))\n",
    "            x = self.pool(self.conv2(x))\n",
    "            x = self.flatten(x)\n",
    "            x = self.fc1(x)\n",
    "            x=self.fc2(x)\n",
    "            return self.fc3(x) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "def computer_cumulative_reward(rewards, max_t,gamma):\n",
    "        returns = deque(maxlen=max_t)\n",
    "        n_steps = len(rewards)\n",
    "        for t in range(n_steps)[::-1]:\n",
    "          disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "          returns.appendleft( rewards[t]+gamma*disc_return_t)\n",
    "        return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def returns_standardization(returns):\n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        ## eps is the smallest representable float, which is\n",
    "        # added to the standard deviation of the returns to avoid numerical instabilities\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "        return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(optimizer,saved_log_probs, returns , state_values):\n",
    "        \n",
    "        state_values= torch.stack(state_values).squeeze()\n",
    "        advantages = returns - state_values\n",
    "        advantages = [g - val for g, val in zip(returns,state_values)]\n",
    "        advantages = torch.Tensor(advantages).to(device)\n",
    "        losses = []\n",
    "        for log_prob, advantage in zip(saved_log_probs, returns):\n",
    "          losses.append(-log_prob *   advantage)\n",
    "        loss = torch.sum(torch.stack(losses))\n",
    "        print(loss)\n",
    "\n",
    "        # actor_loss = torch.cat(loss).sum()\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_critic(optimizer,rewards,pred):\n",
    "    rewards=torch.tensor(rewards,requires_grad=True)\n",
    "    pred=torch.tensor(pred,requires_grad=True)\n",
    "    loss = F.mse_loss(pred, rewards)  # Mean Squared Error (MSE) Loss\n",
    "    print(loss)\n",
    "    optimizer.zero_grad()  # Clear the gradients\n",
    "    loss.backward()  # Compute the backward pass\n",
    "    optimizer.step() \n",
    "    return loss    # Update the weights based on the computed gradients</s>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def critic_fn(actor,critic,actor_optimizer,critic_optimizer, n_training_episodes, max_t, gamma, print_every=1):\n",
    "    # Help us to calculate the score during the training\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    losses=[]\n",
    "\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "\n",
    "        # Generate an episode\n",
    "        #add code here\n",
    "        log_probs, rewards,states = generate_trajectory(actor,critic, max_t)\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        # calculate the return\n",
    "        returns= computer_cumulative_reward(rewards,max_t,gamma)\n",
    "        \n",
    "\n",
    "        ## standardization of the returns is employed to make training more stable\n",
    "        returns=returns_standardization(returns)\n",
    "        # print(returns)\n",
    "\n",
    "        # Train the Critic network\n",
    "        #add code here\n",
    "        loss=train_critic(critic_optimizer, returns,states)\n",
    "        train(actor_optimizer,log_probs,returns,states)\n",
    "        losses.append(loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Actor(actor,optimizer, n_training_episodes, max_t, gamma, print_every=1):\n",
    "    # Help us to calculate the score during the training\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "\n",
    "        # Generate an episode\n",
    "        #add code here\n",
    "        saved_log_probs, rewards = generate_trajectory(actor, max_t)\n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        # calculate the return\n",
    "        returns= computer_cumulative_reward(rewards,max_t,gamma)\n",
    "        \n",
    "\n",
    "        ## standardization of the returns is employed to make training more stable\n",
    "        returns=returns_standardization(returns)\n",
    "        print(returns)\n",
    "\n",
    "        # Train the Critic network\n",
    "        #add code here\n",
    "        train(optimizer,saved_log_probs, returns)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # if i_episode % print_every == 0:\n",
    "        #     print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agent=agent(board)\n",
    "arr=Agent.board_to_array()\n",
    "actor.sample(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a=torch.tensor([0,1,0,0,0]).float()\n",
    "# sampled_indices = torch.multinomial(a,1)\n",
    "# sampled_indices[0].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor=Net()\n",
    "critic=Critic()\n",
    "actor_optimizer = torch.optim.Adam(actor.parameters())\n",
    "critic_optimizer = torch.optim.Adam(critic.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training_episodes= 500\n",
    "max_t = 50\n",
    "gamma = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history=critic_fn(actor,critic,actor_optimizer,critic_optimizer,n_training_episodes,max_t,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(critic.state_dict(),'critic.pth')\n",
    "torch.save(actor.state_dict(),'actor.pth')\n",
    "# scores=Actor(model , optimizer , n_training_episodes , max_t , gamma )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "scores= pd.Series(loss_history, name=\"scores_Actor\")\n",
    "scores.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "_ = scores.plot(ax=ax, label=\"scores_Actor\")\n",
    "_ = (scores.rolling(window=100)\n",
    "           .mean()\n",
    "           .rename(\"Rolling Average\")\n",
    "           .plot(ax=ax))\n",
    "ax.legend()\n",
    "_ = ax.set_xlabel(\"Episode Number\")\n",
    "_ = ax.set_ylabel(\"scores_Actor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Agent=agent(board)\n",
    "generate_trajectory(actor,critic,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list('aaaa')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
